{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Textual Data Handling with NLTK.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaCUGz4XK2P6",
        "outputId": "49c4296b-a6be-458c-bd5b-102a30d6f589"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvubUjG_Ks3V",
        "outputId": "285a5e8f-25ec-400b-87f7-fdef7864ceba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone', '.', 'Welcome', 'to', 'Word', 'Tokenization']\n"
          ]
        }
      ],
      "source": [
        "import nltk.tokenize as nt\n",
        "text = \"Hello everyone. Welcome to Word Tokenization\"\n",
        "words = nt.word_tokenize(text)\n",
        "print(words) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.tokenize as nt\n",
        "\n",
        "text = \"Hello everyone. Welcome to Word Tokenization\"\n",
        "tokenizer = nt.TreebankWordTokenizer()\n",
        "words = tokenizer.tokenize(text)\n",
        "print(words) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_9AAHGnKxaW",
        "outputId": "4ec621f1-32c9-40a5-d45e-19411e433e61"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyone.', 'Welcome', 'to', 'Word', 'Tokenization']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VygVy70jK0HO",
        "outputId": "9d102439-78aa-4cb4-d72e-83b1f0c776f6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'who', 'will', 'ain', 'they', 'ma', 'such', 'her', 'wasn', 'it', 's', 'are', \"won't\", 'were', 'both', 'won', \"you'd\", 'themselves', 'needn', \"should've\", 'weren', 'own', 'him', 'that', \"hasn't\", 'mightn', 'yourselves', 'why', 'before', 'up', 'myself', 'his', 'nor', 'all', 'as', 'not', 'very', 'during', \"that'll\", 'hadn', 'over', 'couldn', \"didn't\", 'now', \"hadn't\", 'other', 'few', 'no', 'haven', 'our', 'aren', 'is', 'here', 'its', 'you', 'then', 'i', 'should', 'having', 'did', 'how', 'ourselves', 'does', 'he', 'below', 'itself', 'through', 'any', 'with', 'doesn', \"aren't\", 'wouldn', 'in', \"isn't\", 'be', \"wouldn't\", 'isn', 'if', 'whom', 'yours', \"you'll\", 'above', 'don', 'll', 'too', 'do', 'against', \"shan't\", 'an', 'doing', 'being', 'which', 'again', 'and', 'where', 'just', 're', 'd', 'theirs', 'am', 'has', 'me', 'most', 'them', \"couldn't\", \"you've\", 'can', 'those', 'yourself', 'm', \"she's\", 'down', 'been', 'for', 'out', 'their', \"it's\", \"haven't\", 'there', 'between', 'some', 'y', \"don't\", 'about', 'she', \"shouldn't\", 'than', 'when', 'a', 'himself', 'these', 'your', 'further', \"needn't\", \"you're\", 'at', 'so', 'off', 'under', 'more', 'because', \"doesn't\", 'once', 't', 'on', 'we', 'hasn', 'hers', 'after', 'this', 'have', 'same', 've', 'to', \"mightn't\", \"weren't\", 'my', 'the', 'by', 'each', \"mustn't\", 'into', 'only', 'didn', 'until', 'from', 'had', 'what', \"wasn't\", 'was', 'or', 'ours', 'shan', 'shouldn', 'mustn', 'o', 'while', 'but', 'herself', 'of'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = \"There was no chance for any character development they were busy\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = word_tokenize(text) \n",
        "result = [] \n",
        "for w in words:\n",
        "    if w not in stop_words:\n",
        "        result.append(w)\n",
        " \n",
        "print(words)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50jPmWQlLH8g",
        "outputId": "2bb35eb1-0a21-493b-8a9c-f1046bc61eb6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'was', 'no', 'chance', 'for', 'any', 'character', 'development', 'they', 'were', 'busy']\n",
            "['There', 'chance', 'character', 'development', 'busy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "ps =  PorterStemmer()\n",
        "ls = LancasterStemmer()\n",
        "words = [\"program\", \"programs\", \"programmer\", \"programming\", \"destabilize\"]\n",
        "print('Porter Stemmer : ')\n",
        "for w in words:\n",
        "  print(w,\" : \",ps.stem(w))\n",
        "print('\\n Lancaster Stemmer : ')\n",
        "for w in words:\n",
        "  print(w,\" : \",ls.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odyHqKD1LTi_",
        "outputId": "a1f59192-cd17-4689-fc6b-53c6b15c2762"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer : \n",
            "program  :  program\n",
            "programs  :  program\n",
            "programmer  :  programm\n",
            "programming  :  program\n",
            "destabilize  :  destabil\n",
            "\n",
            " Lancaster Stemmer : \n",
            "program  :  program\n",
            "programs  :  program\n",
            "programmer  :  program\n",
            "programming  :  program\n",
            "destabilize  :  dest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk \n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "text = \"\"\"This movie made it into one of my top ten most awful movies.\n",
        "There wasn't a continuous minute where there wasn't a fight with one monster or another. \n",
        "There was no chance for any character development, they were too busy running from one sword fight to another. \n",
        "I had no emotional attachment ( except to the big bad machine ## that wanted to destroy them).\"\"\"\n",
        "\n",
        "data = nltk.sent_tokenize(text)\n",
        "for i in range(len(data)):\n",
        "  data[i] = data[i].lower()\n",
        "  data[i] = re.sub(r'\\W+',' ',data[i])\n",
        "  data[i] = re.sub(r'\\s+',' ',data[i])\n",
        "\n",
        "for i in data:\n",
        "  print(i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVvIona1LXGb",
        "outputId": "733a7e0d-e056-4b50-ee2f-a7d25ae8ce70"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this movie made it into one of my top ten most awful movies \n",
            "there wasn t a continuous minute where there wasn t a fight with one monster or another \n",
            "there was no chance for any character development they were too busy running from one sword fight to another \n",
            "i had no emotional attachment except to the big bad machine that wanted to destroy them \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the vocabulary for Bag of Words model\n",
        "import heapq\n",
        "\n",
        "wordcount = {}\n",
        "for i in data:\n",
        "    words = nltk.word_tokenize(i)\n",
        "    for word in words:\n",
        "        if word not in wordcount.keys():\n",
        "            wordcount[word] = 1\n",
        "        else:\n",
        "            wordcount[word] += 1\n",
        "print(len(wordcount))\n",
        "\n",
        "freq_words = heapq.nlargest(30, wordcount, key=wordcount.get)\n",
        "print(freq_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXM2wJsdLmwT",
        "outputId": "e137567a-774e-46bd-cc07-71af8371c311"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "53\n",
            "['one', 'there', 'to', 'wasn', 't', 'a', 'fight', 'another', 'no', 'this', 'movie', 'made', 'it', 'into', 'of', 'my', 'top', 'ten', 'most', 'awful', 'movies', 'continuous', 'minute', 'where', 'with', 'monster', 'or', 'was', 'chance', 'for']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "for i in data:\n",
        "    vector = []\n",
        "    for word in freq_words:\n",
        "        if word in nltk.word_tokenize(i):\n",
        "            vector.append(1)\n",
        "        else:\n",
        "            vector.append(0)\n",
        "    X.append(vector)\n",
        "X = np.asarray(X)\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxRjMjhlLqBT",
        "outputId": "8b10e9ef-c722-4e64-a579-453c1940c4e6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
            " [1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0]\n",
            " [1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
            " [0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"this is an example to extract n-grams\"\n",
        "\n",
        "# function to generate n-grams \n",
        "def extract_ngrams(data, num):\n",
        "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
        "    return [ ' '.join(grams) for grams in n_grams]\n",
        "\n",
        "# 4-grams \n",
        "print(extract_ngrams(text,4))\n",
        "# 5-grams\n",
        "print(extract_ngrams(text,5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnxmt20SRVPF",
        "outputId": "c3bc6cd1-dcd5-4e3f-d74e-f18552fa0f50"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this is an example', 'is an example to', 'an example to extract', 'example to extract n-grams']\n",
            "['this is an example to', 'is an example to extract', 'an example to extract n-grams']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        " \n",
        "# Function to generate n-grams from sentences.\n",
        "def extract_ngrams(data, num):\n",
        "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
        "    return [ ' '.join(grams) for grams in n_grams]\n",
        " \n",
        "data = 'A class is a blueprint for the object.'\n",
        " \n",
        "print(\"1-gram: \", extract_ngrams(data, 1))\n",
        "print(\"2-gram: \", extract_ngrams(data, 2))\n",
        "print(\"3-gram: \", extract_ngrams(data, 3))\n",
        "print(\"4-gram: \", extract_ngrams(data, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bu4G06LWsdO",
        "outputId": "d1ccc7a5-2186-4b3b-8f61-4cd553e547a5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-gram:  ['A', 'class', 'is', 'a', 'blueprint', 'for', 'the', 'object', '.']\n",
            "2-gram:  ['A class', 'class is', 'is a', 'a blueprint', 'blueprint for', 'for the', 'the object', 'object .']\n",
            "3-gram:  ['A class is', 'class is a', 'is a blueprint', 'a blueprint for', 'blueprint for the', 'for the object', 'the object .']\n",
            "4-gram:  ['A class is a', 'class is a blueprint', 'is a blueprint for', 'a blueprint for the', 'blueprint for the object', 'for the object .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3TObOrnZZE9j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}