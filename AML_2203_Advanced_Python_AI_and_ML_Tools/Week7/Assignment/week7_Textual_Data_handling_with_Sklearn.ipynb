{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Performed By:\n",
    "#### Name: Piyush Bhatia\n",
    "#### ID: C0827347\n",
    "#### Course: DSMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\14379\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\14379\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\14379\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\14379\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\14379\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\14379\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing Libraries\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "from nlpretext import Preprocessor\n",
    "import nltk\n",
    "import pandas\n",
    "import spacy\n",
    "from nltk import ngrams\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from spacy import displacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oe4zjEKGaUCU",
    "outputId": "acb2b155-3009-4b4f-cfac-f1855c057a69"
   },
   "outputs": [],
   "source": [
    "#Importing Data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text = \"\"\"This movie made it into one of my top 10 most awful movies. Horrible. \n",
    "        There wasn't a continuous minute where there wasn't a fight with one monster or another. \n",
    "        There was no chance for any character development, they were too busy running from one sword fight to another. \n",
    "        I had no emotional attachment ( except to the big bad machine ## that wanted to destroy them).\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie made it into one of my top 10 most awful movies. Horrible. There was not a continuous minute where there was not a fight with one monster or another. There was no chance for any character development, they were too busy running from one sword fight to another. I had no emotional attachment ( except to the big bad machine ## that wanted to destroy them).\n"
     ]
    }
   ],
   "source": [
    "# Expanding Contractions\n",
    "expanded_words = []    \n",
    "for word in text.split():\n",
    "    ''''''\n",
    "    # using contractions.fix to expand the shotened words\n",
    "    expanded_words.append(contractions.fix(word))   \n",
    "\n",
    "expanded_text = ' '.join(expanded_words)\n",
    "print(expanded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie made it into one of my top most awful movies Horrible There was not a continuous minute where there was not a fight with one monster or another There was no chance for any character development they were too busy running from one sword fight to another I had no emotional attachment except to the big bad machine that wanted to destroy them\n"
     ]
    }
   ],
   "source": [
    "#Removing Special Character\n",
    "preprocessor = Preprocessor()\n",
    "Formatted_Text = re.sub(r\"[^A-Za-z]+\", ' ', expanded_text)\n",
    "spec_text = preprocessor.run(Formatted_Text)\n",
    "print(spec_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'movie', 'made', 'it', 'into', 'one', 'of', 'my', 'top', 'most', 'awful', 'movies', 'Horrible', 'There', 'was', 'not', 'a', 'continuous', 'minute', 'where', 'there', 'was', 'not', 'a', 'fight', 'with', 'one', 'monster', 'or', 'another', 'There', 'was', 'no', 'chance', 'for', 'any', 'character', 'development', 'they', 'were', 'too', 'busy', 'running', 'from', 'one', 'sword', 'fight', 'to', 'another', 'I', 'had', 'no', 'emotional', 'attachment', 'except', 'to', 'the', 'big', 'bad', 'machine', 'that', 'wanted', 'to', 'destroy', 'them']\n"
     ]
    }
   ],
   "source": [
    "#Tokenizing\n",
    "tokenized_text = nltk.tokenize.word_tokenize(spec_text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie made one top awful movies Horrible There continuous minute fight one monster another There chance character development busy running one sword fight another I emotional attachment except big bad machine wanted destroy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\14379\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Removing Stopwords\n",
    "nltk.download('stopwords')\n",
    "list_of_stop_words = stopwords.words('english')\n",
    "\n",
    "list_of_stop_words[1:10]\n",
    "\n",
    "token_without_stopwords = [i for i in tokenized_text if i not in list_of_stop_words]\n",
    "string = ' '.join(map(str,token_without_stopwords))    \n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer : \n",
      "This  :  thi\n",
      "movie  :  movi\n",
      "made  :  made\n",
      "one  :  one\n",
      "top  :  top\n",
      "awful  :  aw\n",
      "movies  :  movi\n",
      "Horrible  :  horribl\n",
      "There  :  there\n",
      "continuous  :  continu\n",
      "minute  :  minut\n",
      "fight  :  fight\n",
      "one  :  one\n",
      "monster  :  monster\n",
      "another  :  anoth\n",
      "There  :  there\n",
      "chance  :  chanc\n",
      "character  :  charact\n",
      "development  :  develop\n",
      "busy  :  busi\n",
      "running  :  run\n",
      "one  :  one\n",
      "sword  :  sword\n",
      "fight  :  fight\n",
      "another  :  anoth\n",
      "I  :  I\n",
      "emotional  :  emot\n",
      "attachment  :  attach\n",
      "except  :  except\n",
      "big  :  big\n",
      "bad  :  bad\n",
      "machine  :  machin\n",
      "wanted  :  want\n",
      "destroy  :  destroy\n",
      "\n",
      " Lancaster Stemmer : \n",
      "This  :  thi\n",
      "movie  :  movy\n",
      "made  :  mad\n",
      "one  :  on\n",
      "top  :  top\n",
      "awful  :  aw\n",
      "movies  :  movy\n",
      "Horrible  :  horr\n",
      "There  :  ther\n",
      "continuous  :  continu\n",
      "minute  :  minut\n",
      "fight  :  fight\n",
      "one  :  on\n",
      "monster  :  monst\n",
      "another  :  anoth\n",
      "There  :  ther\n",
      "chance  :  chant\n",
      "character  :  charact\n",
      "development  :  develop\n",
      "busy  :  busy\n",
      "running  :  run\n",
      "one  :  on\n",
      "sword  :  sword\n",
      "fight  :  fight\n",
      "another  :  anoth\n",
      "I  :  i\n",
      "emotional  :  emot\n",
      "attachment  :  attach\n",
      "except  :  exceiv\n",
      "big  :  big\n",
      "bad  :  bad\n",
      "machine  :  machin\n",
      "wanted  :  want\n",
      "destroy  :  destroy\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "ps =  PorterStemmer()\n",
    "ls = LancasterStemmer()\n",
    "words = nltk.tokenize.word_tokenize(string)\n",
    "print('Porter Stemmer : ')\n",
    "for w in words:\n",
    "  print(w,\" : \",ps.stem(w))\n",
    "print('\\n Lancaster Stemmer : ')\n",
    "for w in words:\n",
    "  print(w,\" : \",ls.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmed Words: \n",
      "This  :  This\n",
      "movie  :  movie\n",
      "made  :  made\n",
      "one  :  one\n",
      "top  :  top\n",
      "awful  :  awful\n",
      "movies  :  movie\n",
      "Horrible  :  Horrible\n",
      "There  :  There\n",
      "continuous  :  continuous\n",
      "minute  :  minute\n",
      "fight  :  fight\n",
      "one  :  one\n",
      "monster  :  monster\n",
      "another  :  another\n",
      "There  :  There\n",
      "chance  :  chance\n",
      "character  :  character\n",
      "development  :  development\n",
      "busy  :  busy\n",
      "running  :  running\n",
      "one  :  one\n",
      "sword  :  sword\n",
      "fight  :  fight\n",
      "another  :  another\n",
      "I  :  I\n",
      "emotional  :  emotional\n",
      "attachment  :  attachment\n",
      "except  :  except\n",
      "big  :  big\n",
      "bad  :  bad\n",
      "machine  :  machine\n",
      "wanted  :  wanted\n",
      "destroy  :  destroy\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "words = nltk.tokenize.word_tokenize(string)\n",
    "print('Lemmed Words: ')\n",
    "for w in words:\n",
    "  print(w,\" : \",lem.lemmatize(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"This movie made it into one of my top 10 most awful movies. Horrible. \n",
    "        There wasn't a continuous minute where there wasn't a fight with one monster or another. \n",
    "        There was no chance for any character development, they were too busy running from one sword fight to another. \n",
    "        I had no emotional attachment ( except to the big bad machine ## that wanted to destroy them).\"\"\"\n",
    "s_t = text.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This movie made it into one of my top 10 most awful movies',\n",
       " ' Horrible',\n",
       " \" \\n        There wasn't a continuous minute where there wasn't a fight with one monster or another\",\n",
       " ' \\n        There was no chance for any character development, they were too busy running from one sword fight to another',\n",
       " ' \\n        I had no emotional attachment ( except to the big bad machine ## that wanted to destroy them)',\n",
       " '']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "for i in s_t:\n",
    "    l.append(i)\n",
    "    \n",
    "l    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'movie', 'made', 'it', 'into', 'one', 'of', 'my', 'top', 'most', 'awful', 'movies', 'Horrible', 'There', 'was', 'not', 'a', 'continuous', 'minute', 'where', 'there', 'was', 'not', 'a', 'fight', 'with', 'one', 'monster', 'or', 'another', 'There', 'was', 'no', 'chance', 'for', 'any', 'character', 'development', 'they', 'were', 'too', 'busy', 'running', 'from', 'one', 'sword', 'fight', 'to', 'another', 'I', 'had', 'no', 'emotional', 'attachment', 'except', 'to', 'the', 'big', 'bad', 'machine', 'that', 'wanted', 'to', 'destroy', 'them']\n",
      "['This', 'movie', 'awful', 'movies', 'Horrible', 'There', 'continuous', 'minute', 'fight', 'monster', 'There', 'chance', 'character', 'development', 'busy', 'running', 'sword', 'fight', 'I', 'emotional', 'attachment', 'big', 'bad', 'machine', 'wanted', 'destroy']\n"
     ]
    }
   ],
   "source": [
    "#Count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "StopWords = list(_stop_words.ENGLISH_STOP_WORDS)\n",
    "vectorizer = CountVectorizer()\n",
    "tokenizer = vectorizer.build_tokenizer()\n",
    "words = tokenized_text\n",
    "result = [] \n",
    "for w in words:\n",
    "    if w not in StopWords:\n",
    "        result.append(w)\n",
    "print(words)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "{'movie': 17, '10': 0, 'awful': 2, 'movies': 18, 'horrible': 13, 'wasn': 22, 'continuous': 8, 'minute': 15, 'fight': 12, 'monster': 16, 'chance': 6, 'character': 7, 'development': 10, 'busy': 5, 'running': 19, 'sword': 20, 'emotional': 11, 'attachment': 1, 'big': 4, 'bad': 3, 'machine': 14, 'wanted': 21, 'destroy': 9}\n",
      "[[1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 2]\n",
      " [0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0]\n",
      " [0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "StopWords = list(_stop_words.ENGLISH_STOP_WORDS)\n",
    "vectorizer = CountVectorizer(stop_words=StopWords)\n",
    "vectorizer.fit(l)\n",
    "print(len(vectorizer.vocabulary_))\n",
    "print(vectorizer.vocabulary_)\n",
    "vectors = vectorizer.transform(l)\n",
    "print(vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "idf values:\n",
      "10 : 2.252762968495368\n",
      "another : 1.8472978603872037\n",
      "any : 2.252762968495368\n",
      "attachment : 2.252762968495368\n",
      "awful : 2.252762968495368\n",
      "bad : 2.252762968495368\n",
      "big : 2.252762968495368\n",
      "busy : 2.252762968495368\n",
      "chance : 2.252762968495368\n",
      "character : 2.252762968495368\n",
      "continuous : 2.252762968495368\n",
      "destroy : 2.252762968495368\n",
      "development : 2.252762968495368\n",
      "emotional : 2.252762968495368\n",
      "except : 2.252762968495368\n",
      "fight : 1.8472978603872037\n",
      "for : 2.252762968495368\n",
      "from : 2.252762968495368\n",
      "had : 2.252762968495368\n",
      "horrible : 2.252762968495368\n",
      "into : 2.252762968495368\n",
      "it : 2.252762968495368\n",
      "machine : 2.252762968495368\n",
      "made : 2.252762968495368\n",
      "minute : 2.252762968495368\n",
      "monster : 2.252762968495368\n",
      "most : 2.252762968495368\n",
      "movie : 2.252762968495368\n",
      "movies : 2.252762968495368\n",
      "my : 2.252762968495368\n",
      "no : 1.8472978603872037\n",
      "of : 2.252762968495368\n",
      "one : 1.5596157879354227\n",
      "or : 2.252762968495368\n",
      "running : 2.252762968495368\n",
      "sword : 2.252762968495368\n",
      "that : 2.252762968495368\n",
      "the : 2.252762968495368\n",
      "them : 2.252762968495368\n",
      "there : 1.8472978603872037\n",
      "they : 2.252762968495368\n",
      "this : 2.252762968495368\n",
      "to : 1.8472978603872037\n",
      "too : 2.252762968495368\n",
      "top : 2.252762968495368\n",
      "wanted : 2.252762968495368\n",
      "was : 2.252762968495368\n",
      "wasn : 2.252762968495368\n",
      "were : 2.252762968495368\n",
      "where : 2.252762968495368\n",
      "with : 2.252762968495368\n"
     ]
    }
   ],
   "source": [
    "# import required module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "data = l\n",
    "tfidf = TfidfVectorizer()\n",
    "# get tf-df values\n",
    "result = tfidf.fit_transform(data)\n",
    "# get idf values\n",
    "print('\\nidf values:')\n",
    "for ele1, ele2 in zip(tfidf.get_feature_names(), tfidf.idf_):\n",
    "    print(ele1, ':', ele2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Textual Data Handling with sklearn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
