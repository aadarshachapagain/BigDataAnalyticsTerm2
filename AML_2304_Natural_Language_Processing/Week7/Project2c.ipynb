{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1596ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae323e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@USAirways BF has been stuck in CLT all day. I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@united DM sent.  This lack if customer servic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@JetBlue thank you for the information.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@AmericanAir I DMed you my AA &amp;amp; phone #s &amp;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@united @44Stocker my wife Sarah stocker did a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8073</th>\n",
       "      <td>@JetBlue Apparently the pilot had made some an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8074</th>\n",
       "      <td>@united customer service sucks!  They hang up ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8075</th>\n",
       "      <td>@USAirways I paid for my seat.  I expect to be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8076</th>\n",
       "      <td>@united so, not only were you Late Flight, you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8077</th>\n",
       "      <td>@AmericanAir finally called! Can't get met Sea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8078 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  airline_sentiment\n",
       "0     @USAirways BF has been stuck in CLT all day. I...                  0\n",
       "1     @united DM sent.  This lack if customer servic...                  0\n",
       "2               @JetBlue thank you for the information.                  1\n",
       "3     @AmericanAir I DMed you my AA &amp; phone #s &...                  0\n",
       "4     @united @44Stocker my wife Sarah stocker did a...                  0\n",
       "...                                                 ...                ...\n",
       "8073  @JetBlue Apparently the pilot had made some an...                  0\n",
       "8074  @united customer service sucks!  They hang up ...                  0\n",
       "8075  @USAirways I paid for my seat.  I expect to be...                  0\n",
       "8076  @united so, not only were you Late Flight, you...                  0\n",
       "8077  @AmericanAir finally called! Can't get met Sea...                  1\n",
       "\n",
       "[8078 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./airline_review_training_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cb0492b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>clean_document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@USAirways BF has been stuck in CLT all day. I...</td>\n",
       "      <td>0</td>\n",
       "      <td>BF has been stuck in CLT all day. Is the loun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@united DM sent.  This lack if customer servic...</td>\n",
       "      <td>0</td>\n",
       "      <td>DM sent.  This lack if customer service is ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@JetBlue thank you for the information.</td>\n",
       "      <td>1</td>\n",
       "      <td>thank you for the information.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@AmericanAir I DMed you my AA &amp;amp; phone #s &amp;...</td>\n",
       "      <td>0</td>\n",
       "      <td>I DMed you my AA  phone   you can't have some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@united @44Stocker my wife Sarah stocker did a...</td>\n",
       "      <td>0</td>\n",
       "      <td>my wife Sarah stocker did also called but co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8073</th>\n",
       "      <td>@JetBlue Apparently the pilot had made some an...</td>\n",
       "      <td>0</td>\n",
       "      <td>Apparently the pilot had made some announceme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8074</th>\n",
       "      <td>@united customer service sucks!  They hang up ...</td>\n",
       "      <td>0</td>\n",
       "      <td>customer service sucks!  They hang up after w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8075</th>\n",
       "      <td>@USAirways I paid for my seat.  I expect to be...</td>\n",
       "      <td>0</td>\n",
       "      <td>I paid for my seat.  I expect to be able to u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8076</th>\n",
       "      <td>@united so, not only were you Late Flight, you...</td>\n",
       "      <td>0</td>\n",
       "      <td>so, not only were you Late Flight, you broke ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8077</th>\n",
       "      <td>@AmericanAir finally called! Can't get met Sea...</td>\n",
       "      <td>1</td>\n",
       "      <td>finally called! Can't get met Seattle so refu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8078 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  airline_sentiment  \\\n",
       "0     @USAirways BF has been stuck in CLT all day. I...                  0   \n",
       "1     @united DM sent.  This lack if customer servic...                  0   \n",
       "2               @JetBlue thank you for the information.                  1   \n",
       "3     @AmericanAir I DMed you my AA &amp; phone #s &...                  0   \n",
       "4     @united @44Stocker my wife Sarah stocker did a...                  0   \n",
       "...                                                 ...                ...   \n",
       "8073  @JetBlue Apparently the pilot had made some an...                  0   \n",
       "8074  @united customer service sucks!  They hang up ...                  0   \n",
       "8075  @USAirways I paid for my seat.  I expect to be...                  0   \n",
       "8076  @united so, not only were you Late Flight, you...                  0   \n",
       "8077  @AmericanAir finally called! Can't get met Sea...                  1   \n",
       "\n",
       "                                         clean_document  \n",
       "0      BF has been stuck in CLT all day. Is the loun...  \n",
       "1      DM sent.  This lack if customer service is ge...  \n",
       "2                        thank you for the information.  \n",
       "3      I DMed you my AA  phone   you can't have some...  \n",
       "4       my wife Sarah stocker did also called but co...  \n",
       "...                                                 ...  \n",
       "8073   Apparently the pilot had made some announceme...  \n",
       "8074   customer service sucks!  They hang up after w...  \n",
       "8075   I paid for my seat.  I expect to be able to u...  \n",
       "8076   so, not only were you Late Flight, you broke ...  \n",
       "8077   finally called! Can't get met Seattle so refu...  \n",
       "\n",
       "[8078 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning Training Data set\n",
    "import re\n",
    "training_data_clean = (\n",
    "    # start with training data\n",
    "    df\n",
    "    # remove all the '\\n' values with space\n",
    "     .assign(clean_document= lambda x: [re.sub(r\"&amp;\",\"\",text) for text in x.text])\n",
    "    #Remove “n\\”\n",
    "    .assign(clean_document= lambda x: [re.sub(r\"\\n\",\"\",text) for text in x.clean_document] )\n",
    "    #Remove mentions i.e. any alphanumeric starting with “@”\n",
    "    .assign(clean_document= lambda x: [re.sub(\"@[A-Za-z0-9_]+\",\"\",text) for text in x.clean_document])\n",
    "    #Remove all hashtags i.e. “#”\n",
    "    .assign(clean_document= lambda x: [re.sub(\"#[A-Za-z0-9_]+\",\"\",text) for text in x.clean_document])\n",
    "#     \n",
    "#     .assign(clean_document= lambda x: [re.sub(r\"#\",\"\",text) for text in x.clean_document])\n",
    "#    Remove all links i.e. any alphanumeric starting with https or http\n",
    "    .assign(clean_document= lambda x: [re.sub(r\"http\\S+\",\"\",text) for text in x.clean_document])   \n",
    ")\n",
    "training_data_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f395b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       1\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "8073    0\n",
       "8074    0\n",
       "8075    0\n",
       "8076    0\n",
       "8077    1\n",
       "Name: airline_sentiment, Length: 8078, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Extraction\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features = 60)\n",
    "# fit_transform(raw_documents[, y])===>Learn vocabulary and idf, return document-term matrix.\n",
    "X = vectorizer.fit_transform([i for i in training_data_clean['clean_document']])\n",
    "df_tfid_sklearn = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())\n",
    "Y_train = training_data_clean['airline_sentiment']\n",
    "X_train =df_tfid_sklearn.iloc[:, :-1] # aLL ROWS EXCEPT LAST\n",
    "Y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fecd1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the classifier & predicting on test data\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "# Learn using fit method\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5292b58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "3458    1\n",
       "3459    0\n",
       "3460    0\n",
       "3461    0\n",
       "3462    0\n",
       "Name: airline_sentiment, Length: 3463, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning the test Data\n",
    "test_data = pd.read_csv(\"./airline_review_test_data.csv\")\n",
    "test_data\n",
    "test_data_clean = (\n",
    "    # start with training data\n",
    "    test_data\n",
    "    # remove all the '\\n' values with space\n",
    "     .assign(clean_document= lambda x: [re.sub(r\"&amp;\",\"\",text) for text in x.text])\n",
    "    #Remove “n\\”\n",
    "    .assign(clean_document= lambda x: [re.sub(r\"\\n\",\"\",text) for text in x.clean_document] )\n",
    "    #Remove mentions i.e. any alphanumeric starting with “@”\n",
    "    .assign(clean_document= lambda x: [re.sub(\"@[A-Za-z0-9_]+\",\"\",text) for text in x.clean_document])\n",
    "    #Remove all hashtags i.e. “#”\n",
    "    .assign(clean_document= lambda x: [re.sub(\"#[A-Za-z0-9_]+\",\"\",text) for text in x.clean_document])\n",
    "#     \n",
    "#     .assign(clean_document= lambda x: [re.sub(r\"#\",\"\",text) for text in x.clean_document])\n",
    "#    Remove all links i.e. any alphanumeric starting with https or http\n",
    "    .assign(clean_document= lambda x: [re.sub(r\"http\\S+\",\"\",text) for text in x.clean_document])   \n",
    ")\n",
    "test_data_clean\n",
    "\n",
    "# use same vectorizer to transform the data\n",
    "# transform(raw_documents)================>Transform documents to document-term matrix.\n",
    "X_test = vectorizer.transform([i for i in test_data_clean['clean_document']])\n",
    "df_tfid_sklearn_test = pd.DataFrame(X_test.toarray(),columns=vectorizer.get_feature_names())\n",
    "X_test =df_tfid_sklearn_test.iloc[:, :-1] # aLL ROWS EXCEPT LAST\n",
    "Y_test =df_tfid_sklearn_test.iloc[:, -1] # lAST row\n",
    "Y_test = Y_test.astype(int)\n",
    "test_data_clean['airline_sentiment']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6288729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>at</th>\n",
       "      <th>be</th>\n",
       "      <th>been</th>\n",
       "      <th>but</th>\n",
       "      <th>can</th>\n",
       "      <th>cancelled</th>\n",
       "      <th>...</th>\n",
       "      <th>up</th>\n",
       "      <th>us</th>\n",
       "      <th>was</th>\n",
       "      <th>we</th>\n",
       "      <th>what</th>\n",
       "      <th>when</th>\n",
       "      <th>why</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.425578</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.371283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352969</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.349927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.319869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.436946</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3458</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.36203</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3459</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.415854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.52088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3460</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.550656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3461</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.393837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.392894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.420138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.473105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.373844</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3463 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      all        an       and       are        at   be      been       but  \\\n",
       "0     0.0  0.000000  0.311435  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "1     0.0  0.000000  0.000000  0.000000  1.000000  0.0  0.000000  0.000000   \n",
       "2     0.0  0.000000  0.250795  0.000000  0.000000  0.0  0.000000  0.357448   \n",
       "3     0.0  0.000000  0.000000  0.356425  0.000000  0.0  0.000000  0.000000   \n",
       "4     0.0  0.000000  0.319869  0.000000  0.436946  0.0  0.000000  0.000000   \n",
       "...   ...       ...       ...       ...       ...  ...       ...       ...   \n",
       "3458  0.0  0.000000  0.222394  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "3459  0.0  0.000000  0.000000  0.000000  0.415854  0.0  0.000000  0.000000   \n",
       "3460  0.0  0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "3461  0.0  0.393837  0.000000  0.000000  0.000000  0.0  0.392894  0.000000   \n",
       "3462  0.0  0.420138  0.000000  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "\n",
       "           can  cancelled  ...       up       us  was        we      what  \\\n",
       "0     0.000000   0.000000  ...  0.00000  0.00000  0.0  0.000000  0.000000   \n",
       "1     0.000000   0.000000  ...  0.00000  0.00000  0.0  0.000000  0.000000   \n",
       "2     0.000000   0.371283  ...  0.00000  0.00000  0.0  0.000000  0.000000   \n",
       "3     0.000000   0.352969  ...  0.00000  0.00000  0.0  0.349927  0.000000   \n",
       "4     0.450728   0.000000  ...  0.00000  0.00000  0.0  0.000000  0.000000   \n",
       "...        ...        ...  ...      ...      ...  ...       ...       ...   \n",
       "3458  0.000000   0.000000  ...  0.36203  0.00000  0.0  0.000000  0.363889   \n",
       "3459  0.000000   0.000000  ...  0.00000  0.52088  0.0  0.000000  0.000000   \n",
       "3460  0.000000   0.000000  ...  0.00000  0.00000  0.0  0.000000  0.000000   \n",
       "3461  0.000000   0.000000  ...  0.00000  0.00000  0.0  0.000000  0.000000   \n",
       "3462  0.000000   0.000000  ...  0.00000  0.00000  0.0  0.000000  0.000000   \n",
       "\n",
       "      when       why  will      with  you  \n",
       "0      0.0  0.000000   0.0  0.425578  0.0  \n",
       "1      0.0  0.000000   0.0  0.000000  0.0  \n",
       "2      0.0  0.000000   0.0  0.000000  0.0  \n",
       "3      0.0  0.000000   0.0  0.000000  0.0  \n",
       "4      0.0  0.000000   0.0  0.000000  0.0  \n",
       "...    ...       ...   ...       ...  ...  \n",
       "3458   0.0  0.000000   0.0  0.000000  0.0  \n",
       "3459   0.0  0.000000   0.0  0.000000  0.0  \n",
       "3460   0.0  0.550656   0.0  0.000000  0.0  \n",
       "3461   0.0  0.000000   0.0  0.000000  0.0  \n",
       "3462   0.0  0.473105   0.0  0.373844  0.0  \n",
       "\n",
       "[3463 rows x 59 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a109b4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "3458    0\n",
       "3459    0\n",
       "3460    0\n",
       "3461    0\n",
       "3462    0\n",
       "Name: your, Length: 3463, dtype: int32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = Y_test.astype(int)\n",
    "Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cda7029a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Clasification report for MNB\n",
      "\n",
      " Accuracy:  0.9301183944556742\n",
      "\n",
      "Classification Report\n",
      "======================================================\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96      3461\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.93      3463\n",
      "   macro avg       0.50      0.47      0.48      3463\n",
      "weighted avg       1.00      0.93      0.96      3463\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction and Classification Report\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "# Classification metrics\n",
    "\n",
    "def generate_classrpt(Y_test, y_pred,algo=''):\n",
    "    if algo!= \"\":\n",
    "        print(\" Clasification report for {algo}\".format(algo=algo))\n",
    "    classification_report_= classification_report(Y_test, y_pred)\n",
    "    print('\\n Accuracy: ', accuracy_score(Y_test, y_pred))\n",
    "    print('\\nClassification Report')\n",
    "    print('======================================================')\n",
    "    print('\\n', classification_report_)\n",
    "    return True\n",
    "\n",
    "generate_classrpt(Y_test, y_pred,algo='MNB')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83be8528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9301183944556742\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "import joblib\n",
    "# save the model to disk\n",
    "filename = 'finalized_MNB_model.sav'\n",
    "joblib.dump(classifier, filename)\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = joblib.load('finalized_MNB_model.sav')\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93a6726b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: TextBlob in e:\\anaconda\\envs\\nlp_env\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in e:\\anaconda\\envs\\nlp_env\\lib\\site-packages (from TextBlob) (3.7)\n",
      "Requirement already satisfied: joblib in e:\\anaconda\\envs\\nlp_env\\lib\\site-packages (from nltk>=3.1->TextBlob) (1.1.0)\n",
      "Requirement already satisfied: click in e:\\anaconda\\envs\\nlp_env\\lib\\site-packages (from nltk>=3.1->TextBlob) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\anaconda\\envs\\nlp_env\\lib\\site-packages (from nltk>=3.1->TextBlob) (2022.3.2)\n",
      "Requirement already satisfied: tqdm in e:\\anaconda\\envs\\nlp_env\\lib\\site-packages (from nltk>=3.1->TextBlob) (4.63.0)\n",
      "Requirement already satisfied: colorama in e:\\anaconda\\envs\\nlp_env\\lib\\site-packages (from click->nltk>=3.1->TextBlob) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# TASK2\n",
    "!pip install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4fae4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "certifi==2021.10.8\n",
      "click @ file:///D:/bld/click_1645238350348/work\n",
      "colorama @ file:///home/conda/feedstock_root/build_artifacts/colorama_1602866480661/work\n",
      "joblib @ file:///home/conda/feedstock_root/build_artifacts/joblib_1633637554808/work\n",
      "nltk==3.7\n",
      "regex @ file:///D:/bld/regex_1646210304648/work\n",
      "textblob==0.17.1\n",
      "tqdm @ file:///home/conda/feedstock_root/build_artifacts/tqdm_1646031859244/work\n",
      "wincertstore==0.2\n"
     ]
    }
   ],
   "source": [
    "!pip freeze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbc0a2b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8e6fa4281b16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# it accepts single text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# for a corpus, we need to run it in a loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "# it accepts single text\n",
    "# for a corpus, we need to run it in a loop\n",
    "\n",
    "# Polarity score ranges from -1 to 1 \n",
    "# -1 means very negative , 0 means nutral and +1 means positive sentiment\n",
    "\n",
    "\n",
    "corpus = ['I am happy','I am very happy', 'I am HAPPY' , 'I am happy but sad too','that SUX' ,'I am happy!!!!!']\n",
    "\n",
    "for text in corpus:\n",
    " score = TextBlob(text).sentiment[0]\n",
    " print (f\"{text}: Sentiment score, {score:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a6f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78341435",
   "metadata": {},
   "outputs": [],
   "source": [
    "strr = 'https://stackoverflow.com/questions/3559559/how-to-delete-a-character-from-a-string-using-python'\n",
    "check  = re.sub(\"http[A-Za-z0-9_]+\",\"\",strr)\n",
    "check2  = re.sub(r\"http\\S+\",\"\",strr)\n",
    "strr2 ='@wertyyy'\n",
    "strr2_check = re.sub(\"@[A-Za-z0-9_]+\",\"\",strr2)\n",
    "# cc=r'http\\S+'\n",
    "\n",
    "\n",
    "print('check2',check2)\n",
    "print('check',check)\n",
    "print('strr2_check:',strr2_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2381784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Reference:\n",
    "https://medium.com/swlh/text-classification-using-tf-idf-7404e75565b8\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "https://datatofish.com/string-to-integer-dataframe/\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html\n",
    "# Saving Model\n",
    "https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/\n",
    "\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bdc945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd94f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
